{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>T2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained IBM1 model that consider French context, and present results of both versions (non-gated and gated). The general setup was to use 1k words, batch_size=25, max_length=20, lr = 0.0005, emb_dim=64, mlp_dim = 128, and Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>\n",
    "<div class=\"plots\">\n",
    "\n",
    "<div class=\"left\" style=\"float:left;width:350px;\">\n",
    "<img src=\"plots/context_accuracy.png\"/>\n",
    "<figcaption style=\"text-align:center\">Non-gated</figcaption>\n",
    "</div>\n",
    "\n",
    "<div class=\"right\" style=\"float:left;width:350px;\">\n",
    "<img src=\"plots/context_gated_accuracy.png\"/>\n",
    "<figcaption style=\"text-align:center\">Gated</figcaption>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loss</h3>\n",
    "<div class=\"plots\">\n",
    "\n",
    "<div class=\"left\" style=\"float:left;width:350px;\">\n",
    "<img src=\"plots/context_loss.png\"/>\n",
    "<figcaption style=\"text-align:center\">Non-gated</figcaption>\n",
    "</div>\n",
    "\n",
    "<div class=\"right\" style=\"float:left;width:350px;\">\n",
    "<img src=\"plots/context_gated_loss.png\"/>\n",
    "<figcaption style=\"text-align:center\">Gated</figcaption>\n",
    "</div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>AER</h3>\n",
    "<div class=\"plots\">\n",
    "\n",
    "<div class=\"left\" style=\"float:left;width:350px;\">\n",
    "<img src=\"plots/context_aer.png\"/>\n",
    "<figcaption style=\"text-align:center\">Non-gated</figcaption>\n",
    "</div>\n",
    "\n",
    "<div class=\"right\" style=\"float:left;width:350px;\">\n",
    "<img src=\"plots/context_gated_aer.png\"/>\n",
    "<figcaption style=\"text-align:center\">Gated</figcaption>\n",
    "</div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also evaluated our trained models on a separate test set, and the results were the following. Non-gated version scored 0.5 aer and 0.32 accuracy. Gated scored 0.99 aer and 0.29 accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "In this setup a non-gated version outperforms a gated one on all metrics. We assume that it can be caused by the fact that modelling relationship between previous French word in terms of a joint transformation is more powerful. On the other hand, we also noticed that modified models perform better than the base one based on accuracy, where the base one scores around 0.48 AER and 0.18 accuracy on validation set. Interestingly, that we did not see significant improvements based on alignments, and in the case of the gated version, the model was converging to a state, where a lot of French words were aligned to a null token. The latter phenomenon can be easily caused by a very limit vocabulary size, or the fact that more complex models do not guarantee better latent assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
