{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "1. [Neural IBM1](#ibm1)\n",
    "    * [Parameterisation](#ibm1-param)\n",
    "    * [MLE](#ibm1-mle)\n",
    "    * [SGD](#ibm1-sgd)\n",
    "        * [Batching](#ibm1-batching)\n",
    "2. [Extensions](#ibm1-ext)\n",
    "    * [Additional French Context](#ibm1-ctxt)\n",
    "        * [Concatenation](#ibm1-ctxt-concat)\n",
    "        * [Gate](#ibm1-ctxt-gate)\n",
    "    * [Latent Gate](#ibm1-latent-gate)\n",
    "        * [Variational Approximation](#ibm1-latent-gate-vi)\n",
    "        * [Reparameterisation](#ibm1-latent-gate-reparam)\n",
    "        * [ELBO](#ibm1-latent-gate-elbo)\n",
    "        * [KL](#ibm1-latent-gate-kl)\n",
    "        * [MC Estimate of ELBO](#ibm1-latent-gate-mc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural IBM1 <a class=\"anchor\" id=\"ibm1\"></a>\n",
    "\n",
    "Conside IBM1's graphical model below, you will be replacing the standard parameterisation with tabular CPDs by deterministic parametric functions.\n",
    "\n",
    "![ibm1](img/ibm1.png)\n",
    "\n",
    "**Variables:**\n",
    "\n",
    "* $F$ is a Categorical random variable taking values from the French vocabulary $V_F$\n",
    "* $E$ is a Categorical random variable taking values from the English vocabulary $V_E$ (extended by a NULL token)\n",
    "* $A$ is a Categorical random variable taking values in the set $0, \\ldots, m$, we call this variable *alignment* is it selects a mixture component (English type) in the English sentence that is used to generate a French word\n",
    "* $\\theta$ is a set of deterministic parameter assignments\n",
    "* throughout, we assume $m$ (the length of the English sentence) to be random, but observed\n",
    "\n",
    "We can now write the joint distribution in terms of the conditional probability distributions (CPDs) in this directed graphical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q1] Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta(f_1^n, a_1^n|e_0^m) &= \\prod_{j=1}^n P_\\theta(f_j, a_j|e_0^m) \\\\\n",
    " &= \\prod_{j=1}^n P_\\theta(a_j|m) P_\\theta(f_j|e_{a_j})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parameterisation <a class=\"anchor\" id=\"ibm1-param\"></a>\n",
    "\n",
    "Throughout, we will assume $P(A|m)$ to always distribute uniformly over $m+1$ events.\n",
    "In this project we will concentrate on the lexical distribution (but you can probably imagine how to extend the argument).\n",
    "\n",
    "IBM1 is parameterised by tabular CPDs, that is, tables of independent (up to a normalisation) probabilities values, where we have one value for each condition-outcome pair.\n",
    "\n",
    "**Tabular CPD:**\n",
    "\n",
    "\\begin{align}\n",
    "P(F|E=e) &= \\mathrm{Cat}(\\theta_e) \\\\\n",
    " &\\quad \\text{where } 0 \\le \\theta_{f|e}\\le 1 \\\\\n",
    " &\\quad \\text{ and } \\sum_{f \\in V_F} \\theta_{f|e} = 1\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "* one parameter $\\theta_{f|e}$ per lexical event\n",
    "* parameters are stored in a table\n",
    "\n",
    "But nothing prevents us from using other parameterisations, for example, a feed-forward network would allow some parameters to be shared across events.\n",
    "\n",
    "**Feed-forward neural network:**\n",
    "\n",
    "\\begin{equation}\n",
    "    P(F|E=e) = \\mathrm{Cat}(t_\\theta(e))\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $t_\\theta(e) = \\mathrm{softmax}(W_t h_E(e) + b_t)$\n",
    "    * note that the softmax is necessary to make $t_\\theta$ produce valid parameters for the categorical distribution\n",
    "    * $W_t \\in \\mathbb R^{|V_F| \\times d_h}$ and $b_t \\in \\mathbb R^{|V_F|}$ \n",
    "* $h_E(e)$ is defined below with $W_{h_E} \\in \\mathbb R^{d_h \\times d_e}$ and $b_{h_E} \\in \\mathbb R^{d_h}$\n",
    "\n",
    "\\begin{equation}\n",
    "h_E(e) = \\underbrace{\\tanh(\\underbrace{W_{h_E} r_E(e) + b_{h_E}}_{\\text{affine}})}_{\\text{elementwise nonlinearity}}\n",
    "\\end{equation}\n",
    "\n",
    "* $r_E(e) = W_{r_E} v_E(e)$ is a word embedding of $e$ with $W_{r_E} \\in \\mathbb R^{d_e \\times |V_E|}$ \n",
    "* $v_E(e) \\in \\{0,1\\}^{v_E} $ is a one-hot encoding of $e$, thus $\\sum_i v_E(e)_i = 1$ \n",
    "* $\\theta = \\{W_t, b_t, W_{h_E}, b_{h_E}, W_{r_E}\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other architectures are also possible, one can use different parameterisations that may use more or less parameters. For example, with a CNN one could make this function sensitive to characters in the words, something along these lines could also be done with RNNs. We will use FFNNs in this project.\n",
    "\n",
    "\n",
    "**Remark on notation**\n",
    "\n",
    "In answering the questions below, use a notation similar to the above one. Also follow the following convention:\n",
    "\n",
    "* $v_F(f)$ for one-hot encoding of $f$\n",
    "* $\\circ$ for vector concatenation\n",
    "* $r_F(f) = W_{r_F} v_F(f)$ with $W_{r_F} \\in \\mathbb R^{d_f \\times |V_F|}$for $f$'s word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLE <a class=\"anchor\" id=\"ibm1-mle\"></a>\n",
    "\n",
    "We can use maximum likelihood estimation (MLE) to choose the parameters of our deterministic function $f_\\theta$. We know at least one general (convex) optimisation algorithm, i.e. *gradient ascent*. This is a gradient-based procedure which chooses $\\theta$ so that the gradient of our objective with respect to $\\theta$ is zero. Even though gradient ascent is meant for convex functions, we often apply it to nonconvex problems. IBM1 would be convex with standard tabular CPDs, but FFNNs with 1 nonlinear hidden layer (or more) make it nonconvex.\n",
    "\n",
    "Nowadays, we have tools that can perform automatic differentiation for us. Thus, for as long as our functions are differentiable, we can get gradients for them rather easily. This is convenient because we get to abstract away from most of the analytical work.\n",
    "\n",
    "Still, some analytical work is on us when working with latent variable models. For example, we still need to be able to express the functional form of the likelihood.\n",
    "\n",
    "Let us then express the log-likelihood (which is the objective we maximise in MLE) of a single sentence pair as a function of our free parameters (it generalises trivially to multiple sentence pairs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q2] Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal L(\\theta|e_0^m, f_1^n) &= \\log P_\\theta(F_1^n=f_1^n|E_0^m = e_0^m) \\\\\n",
    "    &= \\sum_{j=1}^n \\log \\sum_{a_j=0}^m P(a_j|m) P(f_j|e_{a_j})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in fact our log-likelihood is a sum of independent terms $\\mathcal L_j(\\theta|e_0^m, f_j)$, thus we can characterise the contribution of each French word in each sentence pair as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q3] Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L_j(\\theta|e_0^m, f_j) &= \\log P_\\theta(F=f_j|E_0^m = e_0^m) \\\\\n",
    " &= \\log \\sum_{a_j=0}^m P(a_j|m) P(f_j|e_{a_j})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network toolkits usually implement several flavours of gradient-based optimisation for us. But, they are mostly designed as *minimisation* (rather than *maximisation*) algorithms. Thus, we have to work with the idea of a *loss*.\n",
    "\n",
    "To get a loss, we simply negate our objective. \n",
    "\n",
    "You will find a lot of material that mentions some *categorical cross-entropy loss*. \n",
    "\n",
    "\\begin{align}\n",
    "    l(\\theta) &= -\\sum_{(e_0^m, f_1^n)} p_\\star(f_1^n|e_0^m) \\log P_\\theta(f_1^n|e_0^m) \\\\\n",
    "    &\\approx -\\frac{1}{S} \\log P_\\theta(f_1^n|e_0^m)\n",
    "\\end{align}\n",
    "\n",
    "But see that this is just the likelihood of our data assuming that the observations were independently sampled from the true data generating process $p_\\star$.\n",
    "\n",
    "As discussed above, due to the assumptions in our graphical model, this loss factors over individual French positions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q4] Complete this***</span>\n",
    "\n",
    "\\begin{align}\n",
    "    l(\\theta|\\mathcal D) &= -\\frac{1}{S} \\sum_{(e_0^m, f_1^n) \\in \\mathcal D} \\sum_{j}^n \\mathcal L_j(\\theta|e_0^m, f_j) \\\\\n",
    "    &= -\\frac{1}{S} \\sum_{(e_0^m, f_1^n) \\in \\mathcal D} \\sum_{j=1}^n \\log \\sum_{a_j=0}^m P(a_j|m) P(f_j|e_{a_j})\n",
    "\\end{align}\n",
    "\n",
    "Here $\\mathcal D$ is our dataset of $S$ sentence pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 SGD <a class=\"anchor\" id=\"ibm1-sgd\"></a>\n",
    "\n",
    "SGD is really quite simple, we sample a subset $\\mathcal S$ of the training data and compute a loss for that sample. We then use automatic differentiation to obtain a gradient $\\nabla_\\theta \\mathcal l(\\theta|\\mathcal S)$. This gradient is used to update our deterministic parameters $\\theta$.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(t+1)} &= \\theta^{(t)} - \\delta_t \\nabla_{\\theta^{(t)}} l(\\theta^{(t)}|\\mathcal S)\n",
    "\\end{align}\n",
    "\n",
    "The key here is to have a learning rate schedule that complies with a [Robbins and Monro](https://www.jstor.org/stable/2236626) sequence (check [this](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf) for practical notes).\n",
    "Stochastic optimisers are very well studied. Neural network toolkits implement several *well defined* optimisers for you, so using a valid learning rate sequence should not represent much work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q5] Complete this***</span>\n",
    "\n",
    "If $t$ tracks the number of updates, and $\\delta_t$ is the learning rate for update $t$, provide one series that complies with a Robbins and Monro sequence.\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta_{t} &= \\frac{0.1}{t}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Batching (and notes on terminology) <a class=\"anchor\" id=\"ibm1-batching\"></a>\n",
    "\n",
    "In neural network terminology $f_1, \\ldots, f_n$ is a sample and $j$ is a timestep. A collection of samples is a batch. We often work with collections of samples that are much smaller than the full dataset. \n",
    "\n",
    "A note on implementation: \n",
    "* most toolkits deal with static computational graphs, thus we typically have to batch sequences of fixed length (which may require some padding);\n",
    "* padding essentially means that sometimes we will be performing useless computations associated with samples that are not really there, in which case we will be masking (setting to 0) their contributions to the loss as to avoid learning from them.\n",
    "\n",
    "We are providing a tensorflow implementation of this basic neural extension to IBM1.\n",
    "Your task will be to extend it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extensions <a class=\"anchor\" id=\"ibm1-ext\"></a>\n",
    "\n",
    "From here we will discuss a few extensions which you will experiment with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Neural IBM1 (with additional French context) <a class=\"anchor\" id=\"ibm1-ctxt\"></a>\n",
    "\n",
    "\n",
    "Consider the following extension:\n",
    "\n",
    "![ibm1](img/ibm1prev.png)\n",
    "\n",
    "\n",
    "Now that we can use FFNNs to deterministically predict the parameters of our categorical distributions, we can afford conditioning on more events! This model for example generates a French word at position $j$ by conditioning on two events:\n",
    "1. the English word that occupies the position it aligns to\n",
    "2. and the French word in the previous position\n",
    "\n",
    "Let us start by writing the joint distribution, but let us show it for a single French word position (since we can generalise for a whole sentence trivially):\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta(f_j|e_0^m) &= \\sum_{a_j=0}^m P(f_j, a_j|e_0^m, f_{j-1}) \\\\\n",
    " &= \\sum_{a_j=0}^m P(a_j|m) P(f_j|e_{a_j}, f_{j-1}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tabular CPDs, this would be difficult to model due to over-parameterisation.\n",
    "\n",
    "<span style=\"color:red\">***[Q6] Complete this***</span>\n",
    "\n",
    "If $|V_F|$ is the size of the French vocabulary, and $|V_E|$ is the size of the English vocabulary (already counting NULL), then how many free parameters are necessary to model $P(F|E, F_{prev})$ with tabular CPDs?\n",
    "\n",
    "\\begin{align}\n",
    "|V_E| \\cdot (|V_F| - 1)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are going to use FFNNs and make \n",
    "\n",
    "\\begin{equation}\n",
    "P(F|E=e, F_{prev}=f) = \\mathrm{Cat}(t_\\theta(e, f))\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Concatenation <a class=\"anchor\" id=\"ibm1-ctxt-concat\"></a>\n",
    "\n",
    "In a first variant, let's use both observations by **concatenating** their word embeddings.\n",
    "Call $e$ the English word we are aligning to, and call $f$ the previous French word, then\n",
    "\n",
    "1. embed $e$ into $r_E(e)$\n",
    "2. embed $f$ into $r_F(f)$\n",
    "3. concatenate the word embeddings: $r_E(e) \\circ r_F(f)$ \n",
    "3. pass the concatenated embedding through an affine transformation and an elementwise nonlinearity (e.g. tanh)\n",
    "4. predict categorical parameters (i.e. affine transformation followed by softmax)\n",
    "\n",
    "\n",
    "<span style=\"color:red\">***[Q7] Specify r(e,f) according to the recipe above***</span>\n",
    "\n",
    "\n",
    "* $t_\\theta(e, f) = \\mathrm{softmax}(W_t r(e, f) + b_t)$\n",
    "    * $W_t \\in \\mathbb R^{V_F \\times d}$ and $b_t \\in \\mathbb R^{V_F}$ \n",
    "* $r_{EF}(e, f) = \\tanh(W_{r_{EF}} (r_E(e) \\circ r_F(f)) + b_{r_{EF}})$\n",
    "    * $W_{r_{EF}} \\in \\mathbb R^{d \\times d}$ and $b_t \\in \\mathbb R^{d}$\n",
    "* $\\theta = \\{W_t, b_t, W_{r_{EF}}, b_{r_{EF}}, W_{r_E}, W_{r_F}\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Gate <a class=\"anchor\" id=\"ibm1-ctxt-gate\"></a>\n",
    "\n",
    "\n",
    "\n",
    "In a second variant, let's use both words in the context by summing nonlinear transformations of their word embeddings scaled by a **gate value** (a scalar between 0 and 1). Call $e$ the English word we are aligning to, and call $f$ the previous French word, then\n",
    "\n",
    "1. embed $e$ into $r_E(e)$\n",
    "2. embed $f$ into $r_F(f)$\n",
    "3. as a function of the embedding of the previous f, compute a gate value $0 \\le s \\le 1$\n",
    "4. compute a nonlinear transformation of the embedding of e\n",
    "5. compute a nonlinear transformation of the embedding of the previous f\n",
    "6. combine both representations with a weighted sum, where the representation of the previous f gets weighted by the gate value, and the representation of e is weighted by 1 minus the gate value\n",
    "5. from the resulting vector, predict the parameters of the Categorical (that is, affine transformation followed by softmax)\n",
    "\n",
    "<span style=\"color:red\">***[Q8] Specify r(e,f) according to the recipe above***</span>\n",
    "\n",
    "* $t_\\theta(e, f) = \\mathrm{softmax}(W_t r(e, f) + b_t)$\n",
    "    * $W_t \\in \\mathbb R^{V_F \\times d}$ and $b_t \\in \\mathbb R^{V_F}$ \n",
    "* $r(e, f) = s \\cdot h_f + (1-s) \\cdot h_e$\n",
    "* $h_f = \\tanh(W_{h_f} r_F(f) + b_{h_f})$\n",
    "* $h_e = \\tanh(W_{h_e} r_E(e) + b_{h_e})$\n",
    "* $s = W_s r_f(f) + b_s)$\n",
    "* $\\theta = \\{W_t, b_t, W_{h_f}, b_{h_f}, W_{h_e}, b_{h_e}, W_s, b_s\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q9] Complete this***</span>\n",
    "\n",
    "TODO: Discuss the differences between the two parameterisations above and the role of a gate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Neural IBM1 with collocations <a class=\"anchor\" id=\"ibm1-col\"></a>\n",
    "\n",
    "Consider the following extension:\n",
    "\n",
    "![ibm1](img/ibm1c.png)\n",
    "\n",
    "where we have introduce a binary latent variable $c$ which decides between English components and French components. That is, when $c=0$ we generate a French word by *translating* an English word, when $c=1$ we generate a French word by *inserting* it from monolingual (French) context.\n",
    "\n",
    "**Note**:\n",
    "* In comparison to the standard IBM1, French words are now themselves components, and they become available as we progress generating the French string from left-to-right.\n",
    "* In comparison to the previous extension (IBM1 with monolingual context), we incorporate a different type of inductive bias as we give the model the power to explicitly choose between English and French components.\n",
    "* Because we have an explicit latent treatment of this collocation variable, the model will reason with all of its possible assignments. That is, we will effectively marginalise over all options when computing the likelihood of observations (just like we did for alignment).\n",
    "\n",
    "\n",
    "This is the marginal likelihood  (for a single French word position):\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta(f_j|e_0^m) &= \\sum_{a_j=0}^m P(a_j|m) \\left( \\sum_{c_j=0}^1 P(c_j|f_{j-1})P(f_j|e_{a_j}, f_{j-1}) \\right)\\\\\n",
    " &= \\sum_{a_j=0}^m P(a_j|m) \\left(P(C_j=0|F_{j-1}=f_{j-1})P(F_j=f_j|E=e_{a_j}) + P(C_j=1|F_{j-1}=f_{j-1})P(F_j=f_j|F_{j-1}=f_{j-1})\\right) \\\\\n",
    " &= \\sum_{a_j=0}^m P(a_j|m) \\left((1 - s_j)\\times P(f_j|e_{a_j}) + s_j \\times P(f_j|f_{j-1})\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $s_j = P(C_j=1|F_{j-1}=f_{j-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we have 3 CPDs (ignoring the uniform alignment distribution).\n",
    "1. $P(C|F_{\\text{prev}})$ is a distribution over component types (translation vs insertion)\n",
    "2. $P(F|F_{\\text{prev}})$ is a distribution over French words inserted from context\n",
    "3. $P(F|E)$ is the usual lexical translation distribution\n",
    "\n",
    "Now you should be able to use simple FFNNs to parameterise those distributions.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">***[Q10] Complete this***</span>\n",
    "\n",
    "\\begin{equation}\n",
    "P(C|F_{prev}=f) = \\mathrm{Bernoulli}(s_\\theta(f))\n",
    "\\end{equation}\n",
    "\n",
    "* $s_\\theta(f) = \\mathrm{sigmoid}(\\ldots) $\n",
    "    * note that our FFNN will predict a single number which is the parameter of the [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution), this parameter should be a single number between 0 and 1, that's why we use a sigmoid\n",
    "    * TODO: ...\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(F|F_{prev}=f) = \\textrm{Cat}(i_\\theta(f))\n",
    "\\end{equation}\n",
    "\n",
    "* $i_\\theta(f) = \\mathrm{softmax}(\\ldots) $\n",
    "* ...\n",
    "\n",
    "\\begin{equation}\n",
    "P(F|E=e) = \\textrm{Cat}(t_\\theta(e))\n",
    "\\end{equation}\n",
    "\n",
    "* $t_\\theta(e) = \\mathrm{softmax}(\\ldots) $\n",
    "* ...\n",
    "\n",
    "Parameters:\n",
    "* $\\theta = \\{\\ldots\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard NN models would use a *deterministic* gate value in place of this random collocation indicator. \n",
    "\n",
    "For a deterministic we do not have any marginalisation to compute. It also has weaker inductive bias. However, at least in MT, there is a lot of empirical evidence that somehow *soft* decisions (such as this blend of translation and insertion given by a gate value) performs better than *hard* decisions (as the discrete decision of either inserting or translating). We will see a *stochastic* extension of the gate value pretty soon.\n",
    "\n",
    "For now, can you comment on pros/cons of the stochastic view with discrete random variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q11] Complete this***</span>\n",
    "\n",
    "TODO:\n",
    "1. Pros:\n",
    "2. Cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural IBM1 with collocations: latent gate <a class=\"anchor\" id=\"ibm1-latent-gate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this last extension:\n",
    "\n",
    "![ibm1](img/ibm1s.png)\n",
    "\n",
    "here we made the collocation variable $s$ continuous. You can interpret $S$ as a random variable over gate values, thus this model offers a stochastic treatment to [deterministic gates](#ibm1-ctxt-gate).\n",
    "\n",
    "The big difference is that, while $C$ was Bernoulli-distributed, $S$ is [Beta-distributed](https://en.wikipedia.org/wiki/Beta_distribution):\n",
    "\n",
    "\\begin{equation} \n",
    "P(S|F_{prev}=f) = \\mathrm{Beta}(a_\\theta(f), b_\\theta(f))\n",
    "\\end{equation}\n",
    "\n",
    "where we make the shape parameters deterministic functions of the previous French observation. Again, we could easily employ FFNNs for that.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q12] Complete this***</span>\n",
    "\n",
    "Specify $a_\\theta(f)$ and $b_\\theta(f)$ using one tanh hidden layer for each\n",
    "\n",
    "* $a_\\theta(f) = \\exp(W_a h_a(f) + b_a)$\n",
    "* $h_a(f) = \\tanh(W_{h_a} r_F(f) + b_{h_a})$\n",
    "\n",
    "and\n",
    "\n",
    "* $b_\\theta(f) = \\exp(W_b h_b(f) + b_b) $\n",
    "* $h_b(f) = \\tanh(W_{h_b} r_F(f) + b_{h_b})$\n",
    "\n",
    "and\n",
    "\n",
    "* $\\theta = \\{W_a, b_a, W_b, b_b, W_{h_a}, b_{h_a}, W_{h_b}b_{h_b} \\}$\n",
    "\n",
    "*Note that Beta's shape parameters are positive numbers, thus we exponentiate the affine transformation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint distribution**\n",
    "\n",
    "Let's have a look at our joint distribution:\n",
    "\n",
    "\\begin{align}\n",
    "P(f_1^n, a_1^n, s_1^n|e_0^m) &= \\prod_{j=1}^n \\underbrace{\\sum_{a_j=0}^m P(a_j|m) \\int p(s_j|f_{j-1})P(f_j|e_{a_j}, f_{j-1}, s_j) \\mathrm{d}s_j}_{P(f_j|e_0^m)}  \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We now have a big problem: our **marginal likelihood** is no longer tractable!\n",
    "\n",
    "\\begin{align}\n",
    "P(f_j|e_0^m) &= \\sum_{a_j=0}^m P(a_j|m) \\int P(s_j|f_{j-1}) P(f_j|e_{a_j}, f_{j-1}, s_j) \\mathrm{d}s_j \n",
    "\\end{align}\n",
    "\n",
    "it involves marginalising over all possible latent gatent values.\n",
    "Before, this wasn't the case because we had a FFNN deterministically predict a single value for the gate. Now we want to reason with all possible values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Variational Approximation <a class=\"anchor\" id=\"ibm1-latent-gate-vi\"></a>\n",
    "\n",
    "Because this is intractable, we will work with a [*variational auto-encoder*](https://arxiv.org/abs/1312.6114) (VAE).\n",
    "As discussed in [class](https://uva-slpl.github.io/nlp2/resources/slides/vae.pdf), with a simpler variational approximation to the posterior, we can get unbiased Monte Carlo (MC) estimates of a lowerbound on the log-likelihood and of its gradient. \n",
    "\n",
    "We want the posterior over a Beta-distributed variable, so in principle we want to use a Beta posterior approximation and make a mean-field assumption. That is, we will approximate the posterior over $S_j$ locally to each French position $j$:\n",
    "\n",
    "\\begin{align}\n",
    "    q_\\phi(S_j|e_0^m, f_1^n) &= \\mathrm{Beta(a_\\phi(f_{j-1}, f_j), b_\\phi(f_{j-1}, f_j))}\n",
    "\\end{align}\n",
    "\n",
    "Here, we are conditioning only on $f_j$ and $f_{j-1}$, but note that we could condition on anything we like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Reparameterisation <a class=\"anchor\" id=\"ibm1-latent-gate-reparam\"></a>\n",
    "\n",
    "In class, we saw a VAE that employed a Gaussian random variable, for which we derived a change of variable (*reparameterisation trick*). \n",
    "That trick is quite different for Beta distributions and requires a bit more maths (because Beta is not location-scale). Here we follow [Nalisnick and Smyth, 2017 (section 3.2)](https://arxiv.org/pdf/1605.06197.pdf) and choose a different parameterisation for the variational approximation, namely, a [Kumaraswamy distribution](https://en.wikipedia.org/wiki/Kumaraswamy_distribution):\n",
    "\n",
    "\\begin{align}\n",
    "    q_\\phi(S_j|e_0^m, f_1^n) &= \\mathrm{Kuma(\\alpha_\\phi(f_{j-1}, f_j), \\beta_\\phi(f_{j-1}, f_j))}\n",
    "\\end{align}\n",
    "\n",
    "This distribution is closely-related to the Beta and it's easier to reparameterise. Importantly, the KL between Kumaraswamy and Beta (necessary for the ELBO) can be closely approximated in closed-form as we will see.\n",
    "\n",
    "We use FFNNs to compute the parameters $\\alpha_\\phi(f_{j-1}, f_j) > 0$ and $\\beta_\\phi(f_{j-1}, f_j) > 0$ as deterministic functions of the previous French word.\n",
    "\n",
    "In sum, our reparemeterisation looks like the following\n",
    "\n",
    "1. sample a uniform number between 0 and 1, i.e. $u \\sim \\mathrm{Uniform}(0, 1)$\n",
    "2. then $s = (1 - u^{\\frac{1}{\\beta}})^{\\frac{1}{\\alpha}}$ is distributed by a $\\mathrm{Kuma}(\\alpha,\\beta)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q13] Complete this***</span>\n",
    "\n",
    "Specify $\\alpha_\\phi(f, f')$ and $\\beta_\\phi(f, f')$ using one tanh hidden layer for each\n",
    "\n",
    "* $\\alpha_\\phi(f, f') = \\exp(W_\\alpha h_\\alpha(f, f') + b_\\alpha)$\n",
    "* $h_\\alpha(f, f') = \\tanh(W_{h_\\alpha} r(f, f') + b_{h_\\alpha})$\n",
    "\n",
    "and\n",
    "\n",
    "* $\\beta_\\phi(f, f') = \\exp(W_\\beta h_\\beta(f, f') + b_\\beta) $\n",
    "* $h_\\beta(f, f') = \\tanh(W_{h_\\beta} r(f, f') + b_{h_\\beta})$\n",
    "\n",
    "and\n",
    "\n",
    "* $r(f, f') =  w \\cdot r_F(f) + (1-w) \\cdot r_F(f')$\n",
    "* $\\phi = \\{W_\\alpha, b_\\alpha, W_\\beta, b_\\beta, W_{h_\\alpha}, b_{h_\\alpha}, W_{h_\\beta}, b_{h_\\beta}\\}$\n",
    "\n",
    "Note that Kumaraswamy's parameters are positive numbers, thus we exponentiate the affine transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q14] Complete this***</span>\n",
    "\n",
    "For a sampled $s$, we can easily compute a distribution over French words $P(F|E=e, F_{prev}=f, S=s)$ using a FFNN similar to the one in [2.1.2](#ibm1-ctxt-gate), but instead of computing the gate value, we use $s$ as the sampled gate value.\n",
    "\n",
    "That is, $P(F|E=e, F_{prev}=f, S=s) = \\mathrm{Cat}(t_\\theta(e, f, s))$ where\n",
    "\n",
    "* $t_\\theta(e, f, s) = \\mathrm{softmax}(W_t(e, f) + b_t)$\n",
    "    * $W_t \\in \\mathbb R^{|V_F| \\times d}$ and $b_t \\in \\mathbb R^{|V_F|}$\n",
    "* $r(e, f) = \\tanh(W_{r_{EF}} (s \\cdot r_E(e) + (1-s) \\cdot r_F(f)) + b_{r_{EF}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the likelihood of a single French word $f_j$ conditioned on one sampled assignment $s_j$ is\n",
    "\n",
    "\\begin{align}\n",
    "P(f_j|e_0^m, s_j) &= \\sum_{a_j=0}^m P(a_j|m) P(f_j|e_{a_j}, f_{j-1}, s_j) \n",
    "\\end{align}\n",
    "\n",
    "this quantity will become necessary when approximating the ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 ELBO <a class=\"anchor\" id=\"ibm1-latent-gate-elbo\"></a>\n",
    "\n",
    "Now, we will employ gradient-based optimisation to obtain a local optimum of the *evidence lower-bound* (ELBO), which for a single French position is\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E_j(\\theta, \\phi|e_0^m, f_j) \n",
    " &= \\mathbb E_{q_j}[\\log P(f_j|e_0^m, f_{j-1}, S_j)] \n",
    "    - \\mathrm{KL}(q_\\phi(S_j|e_0^m, f_1^n) || p_\\theta(S_j|f_{j-1})) \\\\\n",
    " &= \\mathbb E_{q_j}[\\log P(f_j|e_0^m, f_{j-1}, S_j)] \n",
    "    - \\mathrm{KL}(\\mathrm{Kuma(\\alpha_\\phi(f_{j-1}, f_j), \\beta_\\phi(f_{j-1}, f_j))} || \\mathrm{Beta}(a_\\theta(f_{j-1}), b_\\theta(f_{j-1})))\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the KL between the approximation and the prior involves computing KL divergence between a Kumaraswamy and a Beta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 KL <a class=\"anchor\" id=\"ibm1-latent-gate-kl\"></a>\n",
    "\n",
    "The KL divergence between Kumaraswamy and Beta ([see appendix](https://arxiv.org/pdf/1605.06197.pdf)) is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{KL}(\\mathrm{Kuma}(\\alpha, \\beta) || \\mathrm{Beta}(a, b)) \n",
    " &= \\frac{\\alpha - a}{\\alpha}\\left( -\\gamma - \\Psi(\\beta) - \\frac{1}{\\beta} \\right) \\\\\n",
    " &\\quad + \\log (\\alpha \\beta) + \\log B(a, b) - \\frac{\\beta - 1}{\\beta} \\\\\n",
    " &\\quad + (b - 1)\\beta \\sum_{m=1}^\\infty \\frac{1}{m + \\alpha \\beta} B\\left(\\frac{m}{\\alpha}, \\beta \\right)\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "* $B(a,b)$ is the [Beta function](https://en.wikipedia.org/wiki/Beta_function)\n",
    "* $\\Psi(a)$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function)\n",
    "* $\\gamma$ is [Euler's constant](https://en.wikipedia.org/wiki/Euler–Mascheroni_constant)\n",
    "* and the Taylor expansion can be approximated by the first few terms (choose some finite number of terms, e.g. 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 MC Estimate of ELBO <a class=\"anchor\" id=\"ibm1-latent-gate-mc\"></a>\n",
    "\n",
    "Then a single-sample approximation to the ELBO is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E_j(\\theta, \\phi|e_0^m, f_j) \n",
    " &= \\mathbb \\log P(f_j|e_0^m, f_{j-1}, s_j) \n",
    "    \\\\\n",
    " &\\quad - \\mathrm{KL}(\\mathrm{Kuma(\\alpha_\\phi(f_{j-1}, f_j), \\beta_\\phi(f_{j-1}, f_j))} || \\mathrm{Beta}(a_\\theta(f_{j-1}), b_\\theta(f_{j-1})))\n",
    "\\end{align}\n",
    "\n",
    "where  \n",
    "\n",
    "\\begin{align}\n",
    "s_j &= (1 - u^{\\frac{1}{\\beta_\\phi(f_{j-1}, f_j)}})^{\\frac{1}{\\alpha_\\phi(f_{j-1}, f_j)}}\n",
    "\\end{align}\n",
    "\n",
    "and $u \\sim \\mathrm{Uniform}(0, 1)$, and the KL term can be approximated as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">***[Q15] Complete this***</span>\n",
    "\n",
    "Due to mean-field assumptions, the ELBO factors as independent contributions from French positions.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E(\\theta, \\phi|e_0^m, f_1^n) &= \\sum_{j=1}^n \\mathcal E_j(\\theta, \\phi|e_0^m, f_j)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***[Q16] Complete this***</span>\n",
    "\n",
    "Neural network toolkits implement minimisation algorithms, thus what's the loss for a single French word now\n",
    "\n",
    "\\begin{align}\n",
    "l_j(\\theta, \\phi|e_0^m, f_j) &= -\\mathcal E_j(\\theta, \\phi|e_0^m, f_j)\n",
    "\\end{align}\n",
    "\n",
    "and for a complete sentence?\n",
    "\n",
    "\\begin{align}\n",
    "l(\\theta, \\phi|e_0^m, f_1^n) &= \\sum_{j=1}^n \\mathcal E_j(\\theta, \\phi|e_0^m, f_j)\n",
    "\\end{align}\n",
    "\n",
    "and for the batch?\n",
    "\n",
    "\\begin{align}\n",
    "l(\\theta, \\phi|\\mathcal S) &= \\frac{1}{|B|} \\sum_{(e_0^m, f_1^n) \\in \\mathcal B} l(\\theta, \\phi|e_0^m, f_1^n)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
